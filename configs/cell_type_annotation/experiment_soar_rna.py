from src.task import CellTypeAnnotationTaskConfig
from src.pipeline import PipelineConfig, GenerationConfig
from .dataset import (
    soar_rna_0shot_dataset,
)
from .misc import project_path, output_folder, huggingface_token, openai_token
from .slurm import slurm_config, single_gpu_slurm_config, cpu_slurm_config

experiments = dict(
    soar_rna_with_qwen2_1dot5b=CellTypeAnnotationTaskConfig(
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(),
        pipeline=PipelineConfig(
            model_custom_id="qwen2-1.5b-instruct",
            model_name="Qwen/Qwen2-1.5B-Instruct",
            local_ckpt_path=f"{project_path}/datasets/model_cache/Qwen/Qwen2-1.5B-Instruct",
            torch_dtype="bfloat16",
            device_map="auto",
            tokenizer_kwargs={"padding_side": "left"},
            batch_size=16,
        ),
        slurm=slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_qwen2_1dot5b_zero_shot_cot=CellTypeAnnotationTaskConfig(
        promter_name="zero_shot_cot",
        gene_num_limit=8,
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(max_new_tokens=1024),
        pipeline=PipelineConfig(
            model_custom_id="qwen2-1.5b-instruct",
            model_name="Qwen/Qwen2-1.5B-Instruct",
            local_ckpt_path=f"{project_path}/datasets/model_cache/Qwen/Qwen2-1.5B-Instruct",
            torch_dtype="bfloat16",
            device_map="auto",
            model_kwargs={"load_in_4bit": True},
            tokenizer_kwargs={"padding_side": "left"},
            batch_size=8,
        ),
        slurm=slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_qwen2_7b_zero_shot_cot=CellTypeAnnotationTaskConfig(
        promter_name="zero_shot_cot",
        gene_num_limit=8,
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(max_new_tokens=1024),
        pipeline=PipelineConfig(
            model_custom_id="qwen2-7b-instruct",
            model_name="Qwen/Qwen2-7B-Instruct",
            local_ckpt_path=f"{project_path}/datasets/model_cache/Qwen/Qwen2-7B-Instruct",
            torch_dtype="bfloat16",
            device_map="auto",
            model_kwargs={"load_in_4bit": True},
            tokenizer_kwargs={"padding_side": "left"},
            batch_size=8,
        ),
        slurm=slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_qwen2_72b=CellTypeAnnotationTaskConfig(
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(),
        pipeline=PipelineConfig(
            model_custom_id="qwen2-72b-instruct",
            model_name="Qwen/Qwen2-72B-Instruct",
            local_ckpt_path=f"{project_path}/datasets/model_cache/Qwen/Qwen2-72B-Instruct",
            torch_dtype="bfloat16",
            device_map="auto",
            tokenizer_kwargs={"padding_side": "left"},
            batch_size=16,
        ),
        slurm=slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_qwen2_72b_zero_shot=CellTypeAnnotationTaskConfig(
        promter_name="zero_shot",
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(max_new_tokens=1024),
        pipeline=PipelineConfig(
            model_custom_id="qwen2-72b-instruct",
            model_name="Qwen/Qwen2-72B-Instruct",
            local_ckpt_path=f"{project_path}/datasets/model_cache/Qwen/Qwen2-72B-Instruct",
            torch_dtype="bfloat16",
            device_map="auto",
            tokenizer_kwargs={"padding_side": "left"},
            batch_size=8,
        ),
        slurm=slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_qwen2_72b_zero_shot_cot=CellTypeAnnotationTaskConfig(
        promter_name="zero_shot_cot",
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(max_new_tokens=1024),
        pipeline=PipelineConfig(
            model_custom_id="qwen2-72b-instruct",
            model_name="Qwen/Qwen2-72B-Instruct",
            local_ckpt_path=f"{project_path}/datasets/model_cache/Qwen/Qwen2-72B-Instruct",
            torch_dtype="bfloat16",
            device_map="auto",
            tokenizer_kwargs={"padding_side": "left"},
            batch_size=4,
        ),
        slurm=slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_llama3_70b_zero_shot=CellTypeAnnotationTaskConfig(
        promter_name="zero_shot",
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(max_new_tokens=1024),
        pipeline=PipelineConfig(
            model_custom_id="Meta-Llama-3-70B-Instruct",
            model_name="meta-llama/Meta-Llama-3-70B-Instruct",
            local_ckpt_path=f"{project_path}/datasets/model_cache/meta-llama/Meta-Llama-3-70B-Instruct",
            torch_dtype="bfloat16",
            device_map="auto",
            tokenizer_kwargs={"padding_side": "left"},
            batch_size=8,
        ),
        slurm=slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_llama3_70b_zero_shot_cot=CellTypeAnnotationTaskConfig(
        promter_name="zero_shot_cot",
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(max_new_tokens=1024),
        pipeline=PipelineConfig(
            model_custom_id="Meta-Llama-3-70B-Instruct",
            model_name="meta-llama/Meta-Llama-3-70B-Instruct",
            local_ckpt_path=f"{project_path}/datasets/model_cache/meta-llama/Meta-Llama-3-70B-Instruct",
            torch_dtype="bfloat16",
            device_map="auto",
            tokenizer_kwargs={"padding_side": "left"},
            batch_size=4,
            huggingface_token=huggingface_token,
        ),
        slurm=slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_mixtral_8x7B_zero_shot=CellTypeAnnotationTaskConfig(
        promter_name="zero_shot",
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(max_new_tokens=1024),
        pipeline=PipelineConfig(
            model_custom_id="Mixtral-8x7B-Instruct-v0.1",
            model_name="mistralai/Mixtral-8x7B-Instruct-v0.1",
            local_ckpt_path=f"{project_path}/datasets/model_cache/mistralai/Mixtral-8x7B-Instruct-v0.1",
            torch_dtype="bfloat16",
            device_map="auto",
            tokenizer_kwargs={"padding_side": "left"},
            batch_size=4,
            huggingface_token=huggingface_token,
        ),
        slurm=slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_mixtral_8x7B_zero_shot_cot=CellTypeAnnotationTaskConfig(
        promter_name="zero_shot_cot",
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(max_new_tokens=1024),
        pipeline=PipelineConfig(
            model_custom_id="Mixtral-8x7B-Instruct-v0.1",
            model_name="mistralai/Mixtral-8x7B-Instruct-v0.1",
            local_ckpt_path=f"{project_path}/datasets/model_cache/mistralai/Mixtral-8x7B-Instruct-v0.1",
            torch_dtype="bfloat16",
            device_map="auto",
            tokenizer_kwargs={"padding_side": "left"},
            batch_size=4,
            huggingface_token=huggingface_token,
        ),
        slurm=slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_mixtral_8x22B_zero_shot=CellTypeAnnotationTaskConfig(
        promter_name="zero_shot",
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(max_new_tokens=1024),
        pipeline=PipelineConfig(
            model_custom_id="Mixtral-8x22B-Instruct-v0.1",
            model_name="mistralai/Mixtral-8x22B-Instruct-v0.1",
            local_ckpt_path=f"{project_path}/datasets/model_cache/mistralai/Mixtral-8x22B-Instruct-v0.1",
            torch_dtype="bfloat16",
            device_map="auto",
            model_kwargs={"load_in_4bit": True},
            tokenizer_kwargs={"padding_side": "left"},
            batch_size=4,
            huggingface_token=huggingface_token,
        ),
        slurm=slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_mixtral_8x22B_zero_shot_cot=CellTypeAnnotationTaskConfig(
        promter_name="zero_shot_cot",
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(max_new_tokens=1024),
        pipeline=PipelineConfig(
            model_custom_id="Mixtral-8x22B-Instruct-v0.1",
            model_name="mistralai/Mixtral-8x22B-Instruct-v0.1",
            local_ckpt_path=f"{project_path}/datasets/model_cache/mistralai/Mixtral-8x22B-Instruct-v0.1",
            torch_dtype="bfloat16",
            device_map="auto",
            tokenizer_kwargs={"padding_side": "left"},
            batch_size=4,
            huggingface_token=huggingface_token,
        ),
        slurm=slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_deepseek_llm_67b_zero_shot=CellTypeAnnotationTaskConfig(
        promter_name="zero_shot",
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(max_new_tokens=1024),
        pipeline=PipelineConfig(
            model_custom_id="deepseek-llm-67b-chat",
            model_name="deepseek-ai/deepseek-llm-67b-chat",
            local_ckpt_path=f"{project_path}/datasets/model_cache/deepseek-ai/deepseek-llm-67b-chat",
            torch_dtype="bfloat16",
            device_map="auto",
            tokenizer_kwargs={"padding_side": "left"},
            batch_size=4,
            huggingface_token=huggingface_token,
        ),
        slurm=slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_deepseek_llm_67b_zero_shot_cot=CellTypeAnnotationTaskConfig(
        promter_name="zero_shot_cot",
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(max_new_tokens=1024),
        pipeline=PipelineConfig(
            model_custom_id="deepseek-llm-67b-chat",
            model_name="deepseek-ai/deepseek-llm-67b-chat",
            local_ckpt_path=f"{project_path}/datasets/model_cache/deepseek-ai/deepseek-llm-67b-chat",
            torch_dtype="bfloat16",
            device_map="auto",
            tokenizer_kwargs={"padding_side": "left"},
            batch_size=4,
            huggingface_token=huggingface_token,
        ),
        slurm=slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_gpt4_o_mini_zero_shot=CellTypeAnnotationTaskConfig(
        promter_name="zero_shot",
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(max_new_tokens=1024),
        pipeline=PipelineConfig(
            model_custom_id="gpt-4o-mini",
            model_name="gpt-4o-mini-2024-07-18",
            batch_size=4,
            api_time_interval=1,
            openai_token=openai_token,
            pipeline_class_name="ChatGPTCellTypeAnnotationPipeline",
        ),
        slurm=cpu_slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_gpt4_o_mini_zero_shot_cot=CellTypeAnnotationTaskConfig(
        promter_name="zero_shot_cot",
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(max_new_tokens=1024),
        pipeline=PipelineConfig(
            model_custom_id="gpt-4o-mini",
            model_name="gpt-4o-mini-2024-07-18",
            batch_size=4,
            api_time_interval=1,
            openai_token=openai_token,
            pipeline_class_name="ChatGPTCellTypeAnnotationPipeline",
        ),
        slurm=cpu_slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_gpt4_o_zero_shot=CellTypeAnnotationTaskConfig(
        promter_name="zero_shot",
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(max_new_tokens=1024),
        pipeline=PipelineConfig(
            model_custom_id="gpt-4o",
            model_name="gpt-4o-2024-05-13",
            batch_size=4,
            api_time_interval=1,
            openai_token=openai_token,
            pipeline_class_name="ChatGPTCellTypeAnnotationPipeline",
        ),
        slurm=cpu_slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_gpt4_o_zero_shot_cot=CellTypeAnnotationTaskConfig(
        promter_name="zero_shot_cot",
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(max_new_tokens=1024),
        pipeline=PipelineConfig(
            model_custom_id="gpt-4o",
            model_name="gpt-4o-2024-05-13",
            batch_size=4,
            api_time_interval=1,
            openai_token=openai_token,
            pipeline_class_name="ChatGPTCellTypeAnnotationPipeline",
        ),
        slurm=cpu_slurm_config,
        output_folder=output_folder,
    ),
    soar_rna_with_cell2sent=CellTypeAnnotationTaskConfig(
        promter_name="zero_shot",
        dataset=soar_rna_0shot_dataset,
        generation=GenerationConfig(max_new_tokens=256),
        pipeline=PipelineConfig(
            model_custom_id="pythia-160m-c2s",
            model_name="vandijklab/pythia-160m-c2s",
            local_ckpt_path=f"{project_path}/datasets/model_cache/vandijklab/pythia-160m-c2s",
            local_finetuned_ckpt_path=f"{project_path}/outputs/cell2sentence/07122024_161518/vandijklab/pythia-160m-c2s-immune_finetune-2024-07-12_16-15-36",
            torch_dtype="bfloat16",
            device_map="auto",
            batch_size=4,
            huggingface_token=huggingface_token,
            pipeline_class_name="Cell2SentCellTypeAnnotationPipeline",
        ),
        slurm=single_gpu_slurm_config,
        output_folder=output_folder,
    ),
)
